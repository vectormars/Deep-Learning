### Slides
01. [Mini-batch gradient descent](Slides/C2W2L01.pptx)
02. [Understanding mini-batch gradient descent](Slides/C2W2L02.pptx)
03. [Exponentially weighted averages](Slides/C2W2L03.pptx)
04. [Understanding exponentially weighted averages](Slides/C2W2L03b.pptx)
05. [Bias correction in exponentially weighted averages](Slides/C2W2L04.pptx)
06. [Gradient descent with momentum](Slides/C2W2L05.pptx)
07. [RMSprop](Slides/C2W2L06.pptx)
08. [Adam optimization algorithm](Slides/C2W2L07.pptx)
09. [Learning rate decay](Slides/C2W2L08.pptx)
10. [The problem of local optima](Slides/C2W2L09.pptx)

### Programming Assignment:
